{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Setups"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotnine as p9\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import transformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from fairnesTester import FairnessTester\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fairnesTester import FairnessTester\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#pipelines and transformer\n",
    "\n",
    "cat_trans = Pipeline(steps=[\n",
    "    (\"selector\", transformer.DataSelector(\"object\")),\n",
    "    (\"one_hot\", preprocessing.OneHotEncoder())\n",
    "])\n",
    "num_trans = Pipeline(steps=[\n",
    "    (\"selector\", transformer.DataSelector(\"number\")),\n",
    "    (\"scaler\", StandardScaler() )\n",
    "])\n",
    "\n",
    "pre_pipe = FeatureUnion(transformer_list=[\n",
    "    (\"cat\", cat_trans),\n",
    "    (\"num\", num_trans)\n",
    "])\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "del_nan = transformer.DeleteNAN(\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing and preparing the data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adult Income Data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/adult.data\"\n",
    "names = [\"age\", \"workclass\", \"fnlwgt\",\"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
    "train = pd.read_csv(filename, names=names)\n",
    "test = test = pd.read_csv(\"Datasets/adult.test\", names=names)\n",
    "\n",
    "del_nan.set_nan_char(\" ?\")\n",
    "train = del_nan.transform(train)\n",
    "test = del_nan.transform(test)\n",
    "\n",
    "train[\"class\"] = lb.fit_transform(train[\"class\"])\n",
    "test[\"class\"] = lb.fit_transform(test[\"class\"])\n",
    "\n",
    "train_data = pre_pipe.fit_transform(train.drop(\"class\", axis=1))\n",
    "train_labels = train[\"class\"]\n",
    "\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "test_labels = test[\"class\"]\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Adult_Income\"\n",
    "priv_val = \" Male\"\n",
    "unpriv_val = \" Female\"\n",
    "protected_att = \"sex\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## German Credit Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/german.data\"\n",
    "names = [\"status existing account\",\"duration\", \"credit history\", \"purpose\", \"credit amount\", \"savings\", \"employment since\", \"installment rate\", \"sex\", \"other debtors\", \"residence since\", \"property\", \"age\", \"installment plans\", \"housing\", \"num existing credits\", \"job\", \"no of pople liable\", \"telephone\", \"foreign worker\", \"class\" ]\n",
    "data = pd.read_csv(filename, sep=\" \", names =names)\n",
    "\n",
    "data[\"class\"] = lb.fit_transform(data[\"class\"])\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data, data[\"class\"], random_state=42)\n",
    "\n",
    "train_data = pre_pipe.fit_transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "#transform for Fairness tester\n",
    "test[\"sex\"].replace([\"A91\",\"A93\", \"A94\"],\"m\",inplace=True)\n",
    "test[\"sex\"].replace([\"A92\",\"A95\"],\"f\",inplace=True)\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"German_Credit\"\n",
    "priv_val = \"m\"\n",
    "unpriv_val = \"f\"\n",
    "protected_att = \"sex\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Default of Credit Card Payments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/default of credit.xls\"\n",
    "data_inp = pd.read_excel(filename, dtype={\"X1\": int,\"X2\": object,\"X3\": object,\"X4\": object,\"X5\": object,\"X6\": object,\"X7\": object,\"X8\": object,\"X9\": object,\"X10\": object,\"X11\": object,\"X12\": int,\"X13\": int,\"X14\": int,\"X15\": int,\"X16\": int,\"X17\": int,\"X23\": int,\"X18\": int,\"X19\": int,\"X20\": int,\"X21\": int,\"X22\": int})\n",
    "\n",
    "data_inp = data_inp.rename(columns={\"Y\": \"class\"})\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Default_Of_Credit\"\n",
    "priv_val = 1 #male\n",
    "unpriv_val = 2 #female\n",
    "protected_att = \"X2\"\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rici vs Stefano Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/ricci.csv\"\n",
    "data_inp = pd.read_csv(filename).drop(\"Unnamed: 0\", axis=1)\n",
    "#applicants with combine >= 70 pass\n",
    "#read paper Did the Results of Promotion Exams Have a Disparate Impact on Minorities? Using Statistical Evidence in Ricci v. DeStefano\n",
    "data_inp.rename(columns={\"Combine\": \"class\"}, inplace=True)\n",
    "\n",
    "data_inp.loc[(data_inp[\"class\"]<70), \"class\"] = 0\n",
    "data_inp.loc[data_inp[\"class\"]>=70, \"class\"] = 1\n",
    "\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "#transform for Fairness tester\n",
    "test[\"Race\"].replace([\"H\",\"B\",],\"NW\",inplace=True)\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Ricci_vs_Stefano\"\n",
    "priv_val = \"W\" #white\n",
    "unpriv_val = \"NW\" #not white\n",
    "protected_att = \"Race\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heart Disease Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/processed.cleveland.data\"\n",
    "names = [\"age\", \"sex\", 3,4,5,6,7,8,9,10,11,12,13,\"class\"]\n",
    "data_inp = pd.read_csv(filename, names=names)\n",
    "\n",
    "\n",
    "data_inp.loc[data_inp[\"class\"]>=1, \"class\"] = 1 #existing heart disase\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Heart_Diseases\"\n",
    "priv_val = 1 #male\n",
    "unpriv_val = 0 #female\n",
    "protected_att = \"sex\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heart Failure Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/heart_failure.csv\"\n",
    "data_inp = pd.read_csv(filename)\n",
    "data_inp.rename(columns={\"DEATH_EVENT\":\"class\"}, inplace=True)\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Heart_Failure\"\n",
    "priv_val = 1 #male\n",
    "unpriv_val = 0 #female\n",
    "protected_att = \"sex\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Student Performance Data Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/student-por.csv\"\n",
    "data_inp = pd.read_csv(filename, sep=\";\")\n",
    "\n",
    "data_inp.drop([\"G1\",\"G2\"],axis=1,inplace=True)\n",
    "data_inp.rename(columns={\"G3\":\"class\"},inplace=True)\n",
    "\n",
    "data_inp.loc[(data_inp[\"class\"]<10), \"class\"] = 0 #failed\n",
    "data_inp.loc[data_inp[\"class\"]>=10, \"class\"] = 1 #passed\n",
    "\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Student_Performance\"\n",
    "priv_val = \"M\" #male\n",
    "unpriv_val = \"F\" #female\n",
    "protected_att = \"sex\"\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classifiers = [DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42),SVC(),AdaBoostClassifier(),KNeighborsClassifier(5), GaussianNB(), XGBClassifier()]\n",
    "model_names = []\n",
    "\n",
    "for model in classifiers:\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    print(name)\n",
    "    model_names.append(name)\n",
    "\n",
    "    model.fit(train_data.toarray(), train_labels)\n",
    "    pred = model.predict(test_data.toarray())\n",
    "\n",
    "    test[name]=pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing for Fairness\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## testing classifiers list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tester = FairnessTester()\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for name in model_names:\n",
    "    tester.setup(test, protected_att, priv_val, unpriv_val, name)\n",
    "    result_dic = {\"model\": name}\n",
    "    result_dic.update(tester.confusion_based_dic_priv())\n",
    "    result_df= result_df.append(result_dic, ignore_index=True)\n",
    "    \n",
    "    result_dic = {\"model\": name}\n",
    "    result_dic.update(tester.confusion_based_dic_unpriv())\n",
    "    result_df= result_df.append(result_dic, ignore_index=True)\n",
    "definitions_names = list(tester.confuison_based_dic().keys())\n",
    "\n",
    "result_df.to_csv(\"results/\"+dataset_name+\".csv\")\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Results from CSV "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#read all data into a list of results\n",
    "dataset_names = [\"Adult_Income\", \"Default_Of_Credit\", \"German_Credit\", \"Heart_Diseases\", \"Heart_Failure\", \"Ricci_vs_Stefano\", \"Student_Performance\"]\n",
    "definitions_names = [\"statistical parity\", \"predictive parity\", \"negative predictive parity\", \"equal opportunity\", \"predictive equality\", \"overall accuracy equality\", \"treatment equality\"]\n",
    "classifiers = [DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42),SVC(),AdaBoostClassifier(),KNeighborsClassifier(5), GaussianNB(), XGBClassifier()]\n",
    "model_names = []\n",
    "\n",
    "for model in classifiers:\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    model_names.append(name)\n",
    "\n",
    "#create list of results\n",
    "results=[]\n",
    "for dataset in dataset_names:\n",
    "    result = pd.read_csv(\"results/\"+dataset+\".csv\")\n",
    "    results.append(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Results for Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#transform to better selectable format for all datasets\n",
    "full_result_df = pd.DataFrame()\n",
    "i=0\n",
    "for result in results:\n",
    "    result_df = result\n",
    "    for model in model_names:\n",
    "        for defi in definitions_names:\n",
    "            for group in [\"priv\", \"unpriv\"]:    \n",
    "                if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "                    continue \n",
    "                dic = {}\n",
    "                dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "                dic[\"group\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"group\"].item()\n",
    "                dic[\"definition\"] = defi\n",
    "                dic[\"result\"]= result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][defi].item()\n",
    "                dic[\"dataset\"] = dataset_names[i]\n",
    "                full_result_df = full_result_df.append(dic, ignore_index=True)\n",
    "    i+=1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#calculate differences for all datasets\n",
    "\n",
    "full_differences = pd.DataFrame()\n",
    "i=0\n",
    "for result in results:\n",
    "    result_df = result\n",
    "    differences = pd.DataFrame()\n",
    "    for model in model_names:\n",
    "        for defi in definitions_names:   \n",
    "            if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "                continue          \n",
    "            dic = {}\n",
    "            dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "            x = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"priv\")][defi].item()\n",
    "            y = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"unpriv\")][defi].item()\n",
    "            dic[\"definition\"] = defi\n",
    "            diff = abs(x-y)\n",
    "            dic[\"difference\"]= diff\n",
    "            if (diff <=0.1): # implementing threshold for consideration of fairness/unfairness\n",
    "                dic[\"fairness\"]=\"Fair\"\n",
    "            else:\n",
    "                dic[\"fairness\"]=\"Unfair\"\n",
    "            differences = differences.append(dic, ignore_index=True)\n",
    "    differences[\"dataset\"]=dataset_names[i]\n",
    "    full_differences = full_differences.append(differences, ignore_index=True)\n",
    "    \n",
    "    i+=1  \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#overall plot for all definitions with color\n",
    "#skip treatment equality as it will destory the scales\n",
    "for dataset_name in dataset_names:\n",
    "    plot = (p9.ggplot(data= full_result_df.loc[full_result_df[\"dataset\"]==dataset_name], mapping = p9.aes(x=\"model\", y=\"result\", fill=\"group\")) \n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.facet_grid(\".~definition\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Results\", title=(\" Complete results for \" + dataset_name))\n",
    "        )\n",
    "    plot.save(filename=\"plots/\"+dataset_name+\"/\"+dataset_name+\"_complete_color.png\", height=4 , width = 17)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot differences\n",
    "for dataset_name in dataset_names: \n",
    "    plot = (p9.ggplot(data= full_differences.loc[full_differences[\"dataset\"]==dataset_name], mapping = p9.aes(x=\"model\", y=\"difference\", fill=\"fairness\")) \n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.facet_grid(\".~definition\", space=\"free_x\", scales=\"fixed\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Differences for \" + dataset_name))\n",
    "        + p9.ylim(0,1)\n",
    "        + p9.scale_fill_manual(values=(\"green\",\"red\"))\n",
    "        )\n",
    "    plot.save(filename=\"plots/\"+dataset_name+\"/\"+dataset_name+\"_differences_scaled.png\", height=4 , width = 17)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#plot differences for each definition\n",
    "for dataset_name in dataset_names:\n",
    "    plots = []\n",
    "    for definition in definitions_names:\n",
    "\n",
    "        plot = (p9.ggplot(data= full_differences.loc[(full_differences[\"definition\"]==definition)&(full_differences[\"dataset\"]==dataset_name)],\n",
    "                 mapping = p9.aes(x=\"model\", y=\"difference\", fill=\"fairness\")) \n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Differences for \"+definition+\" on \" + dataset_name + \" Dataset\"))\n",
    "        + p9.ylim(0,1)\n",
    "        + p9.scale_fill_manual(values=({\"Fair\":\"green\",\"Unfair\":\"red\"}))\n",
    "        )\n",
    "        plots.append(plot)\n",
    "    i=0\n",
    "\n",
    "    for plot in plots:\n",
    "        if definitions_names[i]==\"treatment equality\":\n",
    "            continue\n",
    "        plot.save(filename=\"plots/\"+dataset_name+\"/\"+dataset_name+\"_\"+definitions_names[i]+\"_difference.png\")\n",
    "        i+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#differences for each definition on all datasets\n",
    "for definition in definitions_names:\n",
    "        if definition==\"treatment equality\":\n",
    "            continue\n",
    "        plot = (p9.ggplot(data= full_differences.loc[(full_differences[\"definition\"]==definition)],\n",
    "                        mapping = p9.aes(x=\"model\", y=\"difference\", fill=\"fairness\")) \n",
    "                + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "                + p9.geom_col(position=\"dodge\")\n",
    "                + p9.facet_wrap(\"dataset\")\n",
    "                + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "                + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Differences for \"+definition))\n",
    "                + p9.ylim(0,1)\n",
    "                + p9.scale_fill_manual(values=({\"Fair\":\"green\",\"Unfair\":\"red\"}))\n",
    "                )\n",
    "        plot.save(filename=\"plots/overall/\"+definition+\"_differences.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#boxplots on classifiers over all datasets for each definition\n",
    "\n",
    "plot = (p9.ggplot(data= full_differences, mapping = p9.aes(x=\"model\", y= \"difference\"))\n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.facet_grid(\".~definition\",space=\"free_x\", scales=\"fixed\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Boxplots for all ML models\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/overall/boxplot_models.png\",height=4 , width = 17)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "#boxplot for definitions\n",
    "plot = (p9.ggplot(data= full_differences, mapping = p9.aes(x=\"definition\", y= \"difference\"))\n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Boxplots for all definitions\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/overall/boxplot_definitions.png\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}