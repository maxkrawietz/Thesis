{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotnine as p9\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import transformer\n",
    "from sklearn import metrics\n",
    "from fairnesTester import FairnessTester\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fairnesTester import FairnessTester\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#pipelines and transformer\n",
    "\n",
    "cat_trans = Pipeline(steps=[\n",
    "    (\"selector\", transformer.DataSelector(\"object\")),\n",
    "    (\"one_hot\", preprocessing.OneHotEncoder())\n",
    "])\n",
    "num_trans = Pipeline(steps=[\n",
    "    (\"selector\", transformer.DataSelector(\"number\")),\n",
    "    (\"scaler\", StandardScaler() )\n",
    "])\n",
    "\n",
    "pre_pipe = FeatureUnion(transformer_list=[\n",
    "    (\"cat\", cat_trans),\n",
    "    (\"num\", num_trans)\n",
    "])\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "del_nan = transformer.DeleteNAN(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and preparing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Income Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Datasets/adult.data\"\n",
    "names = [\"age\", \"workclass\", \"fnlwgt\",\"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
    "train = pd.read_csv(filename, names=names)\n",
    "test = test = pd.read_csv(\"Datasets/adult.test\", names=names)\n",
    "\n",
    "del_nan.set_nan_char(\" ?\")\n",
    "train = del_nan.transform(train)\n",
    "test = del_nan.transform(test)\n",
    "\n",
    "train[\"class\"] = lb.fit_transform(train[\"class\"])\n",
    "test[\"class\"] = lb.fit_transform(test[\"class\"])\n",
    "\n",
    "train_data = pre_pipe.fit_transform(train.drop(\"class\", axis=1))\n",
    "train_labels = train[\"class\"]\n",
    "\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "test_labels = test[\"class\"]\n",
    "\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Adult_Income\"\n",
    "priv_val = \" Male\"\n",
    "unpriv_val = \" Female\"\n",
    "protected_att = \"sex\"\n",
    "\n",
    "datasets.append((test,dataset_name,priv_val,unpriv_val,protected_att, train_data, train_labels,test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German Credit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Datasets/german.data\"\n",
    "names = [\"status existing account\",\"duration\", \"credit history\", \"purpose\", \"credit amount\", \"savings\", \"employment since\", \"installment rate\", \"sex\", \"other debtors\", \"residence since\", \"property\", \"age\", \"installment plans\", \"housing\", \"num existing credits\", \"job\", \"no of pople liable\", \"telephone\", \"foreign worker\", \"class\" ]\n",
    "data = pd.read_csv(filename, sep=\" \", names =names)\n",
    "\n",
    "data[\"class\"] = lb.fit_transform(data[\"class\"])\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data, data[\"class\"], random_state=42)\n",
    "\n",
    "train_data = pre_pipe.fit_transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "#transform for Fairness tester\n",
    "test[\"sex\"].replace([\"A91\",\"A93\", \"A94\"],\"male\",inplace=True)\n",
    "test[\"sex\"].replace([\"A92\",\"A95\"],\"female\",inplace=True)\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"German_Credit\"\n",
    "priv_val = \"male\"\n",
    "unpriv_val = \"female\"\n",
    "protected_att = \"sex\"\n",
    "\n",
    "datasets.append((test,dataset_name,priv_val,unpriv_val,protected_att, train_data, train_labels,test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default of Credit Card Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Datasets/default of credit.xls\"\n",
    "data_inp = pd.read_excel(filename, dtype={\"X1\": int,\"X2\": object,\"X3\": object,\"X4\": object,\"X5\": object,\"X6\": object,\"X7\": object,\"X8\": object,\"X9\": object,\"X10\": object,\"X11\": object,\"X12\": int,\"X13\": int,\"X14\": int,\"X15\": int,\"X16\": int,\"X17\": int,\"X23\": int,\"X18\": int,\"X19\": int,\"X20\": int,\"X21\": int,\"X22\": int})\n",
    "\n",
    "data_inp = data_inp.rename(columns={\"Y\": \"class\"})\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "test[\"X2\"].replace([1],\"male\",inplace=True)\n",
    "test[\"X2\"].replace([2],\"female\",inplace=True)\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Default_Of_Credit\"\n",
    "priv_val = \"male\" #male\n",
    "unpriv_val = \"female\" #female\n",
    "protected_att = \"X2\"\n",
    "\n",
    "datasets.append((test,dataset_name,priv_val,unpriv_val,protected_att, train_data, train_labels,test_data, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rici vs Stefano Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Datasets/ricci.csv\"\n",
    "data_inp = pd.read_csv(filename).drop(\"Unnamed: 0\", axis=1)\n",
    "#applicants with combine >= 70 pass\n",
    "data_inp.rename(columns={\"Combine\": \"class\"}, inplace=True)\n",
    "\n",
    "data_inp.loc[(data_inp[\"class\"]<70), \"class\"] = 0\n",
    "data_inp.loc[data_inp[\"class\"]>=70, \"class\"] = 1\n",
    "\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "#transform for Fairness tester\n",
    "test[\"Race\"].replace([\"H\",\"B\",],\"non-white\",inplace=True)\n",
    "test[\"Race\"].replace([\"W\"],\"white\",inplace=True)\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Ricci_vs_Stefano\"\n",
    "priv_val = \"white\" #white\n",
    "unpriv_val = \"non-white\" #not white\n",
    "protected_att = \"Race\"\n",
    "\n",
    "datasets.append((test,dataset_name,priv_val,unpriv_val,protected_att, train_data, train_labels,test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Datasets/processed.cleveland.data\"\n",
    "names = [\"age\", \"sex\", 3,4,5,6,7,8,9,10,11,12,13,\"class\"]\n",
    "data_inp = pd.read_csv(filename, names=names)\n",
    "\n",
    "\n",
    "data_inp.loc[data_inp[\"class\"]>=1, \"class\"] = 1 #existing heart disase\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "test[\"sex\"].replace([1],\"male\",inplace=True)\n",
    "test[\"sex\"].replace([2],\"female\",inplace=True)\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Heart_Diseases\"\n",
    "priv_val = \"male\" #male\n",
    "unpriv_val = \"female\" #female\n",
    "protected_att = \"sex\"\n",
    "\n",
    "datasets.append((test,dataset_name,priv_val,unpriv_val,protected_att, train_data, train_labels,test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Failure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Datasets/heart_failure.csv\"\n",
    "data_inp = pd.read_csv(filename)\n",
    "data_inp.rename(columns={\"DEATH_EVENT\":\"class\"}, inplace=True)\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "test[\"sex\"].replace([1],\"male\",inplace=True)\n",
    "test[\"sex\"].replace([0],\"female\",inplace=True)\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Heart_Failure\"\n",
    "priv_val = \"male\" #male\n",
    "unpriv_val = \"female\" #female\n",
    "protected_att = \"sex\"\n",
    "\n",
    "datasets.append((test,dataset_name,priv_val,unpriv_val,protected_att, train_data, train_labels,test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Performance Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Datasets/student-por.csv\"\n",
    "data_inp = pd.read_csv(filename, sep=\";\")\n",
    "\n",
    "data_inp.drop([\"G1\",\"G2\"],axis=1,inplace=True)\n",
    "data_inp.rename(columns={\"G3\":\"class\"},inplace=True)\n",
    "\n",
    "data_inp.loc[(data_inp[\"class\"]<10), \"class\"] = 0 #failed\n",
    "data_inp.loc[data_inp[\"class\"]>=10, \"class\"] = 1 #passed\n",
    "\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "test[\"sex\"].replace([\"M\"],\"male\",inplace=True)\n",
    "test[\"sex\"].replace([\"F\"],\"female\",inplace=True)\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Student_Performance\"\n",
    "priv_val = \"male\" #male\n",
    "unpriv_val = \"female\" #female\n",
    "protected_att = \"sex\"\n",
    "\n",
    "datasets.append((test, dataset_name, priv_val, unpriv_val, protected_att, train_data, train_labels,test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baserates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baserates\n",
    "\n",
    "i=0\n",
    "baserates = pd.DataFrame(columns = [\"dataset\", \"Privileged Attribute\", \"Unprivileged Attribute\", \"Total Entrys\", \"Total Rate Privileged\", \"Total Rate Unprivileged\", \"Rate of Positives Privileged\", \"Rate of Positives Unprivileged\" ])\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    test = dataset[0]\n",
    "    dataset_name = dataset[1]\n",
    "    priv_val = dataset[2]\n",
    "    unpriv_val = dataset[3]\n",
    "    protected_att=dataset[4]\n",
    "    \n",
    "    total = test[\"class\"].count()\n",
    "\n",
    "    priv_total = test.loc[(test[protected_att]==priv_val)][\"class\"].count()\n",
    "    priv_total_rate = priv_total/total\n",
    "    priv_positive_rate = test.loc[(test[protected_att]==priv_val)&(test[\"class\"]==1)][\"class\"].count() / priv_total\n",
    "\n",
    "\n",
    "    unpriv_total = test.loc[(test[protected_att]==unpriv_val)][\"class\"].count()\n",
    "    unpriv_total_rate = unpriv_total/total\n",
    "    unpriv_positive_rate = test.loc[(test[protected_att]==unpriv_val)&(test[\"class\"]==1)][\"class\"].count() / priv_total\n",
    "\n",
    "    baserates.loc[len(baserates)]=[dataset_name,priv_val,unpriv_val,total,priv_total_rate,unpriv_total_rate,priv_positive_rate,unpriv_positive_rate ]\n",
    "print(baserates.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up list of classifiers\n",
    "classifiers = [DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42),SVC(),AdaBoostClassifier(),KNeighborsClassifier(5), GaussianNB(), XGBClassifier()]\n",
    "model_names = []\n",
    "for model in classifiers:\n",
    "    name = model.__class__.__name__\n",
    "    model_names.append(name)\n",
    "#train and run classifiers and safe prediction in dataframe\n",
    "for dataset in datasets:\n",
    "    print(dataset[1])\n",
    "    train_data = dataset[5]\n",
    "    train_labels = dataset[6]\n",
    "    test_data = dataset[7]\n",
    "    \n",
    "    for model in classifiers:\n",
    "        \n",
    "        name = model.__class__.__name__\n",
    "        print(name)\n",
    "        \n",
    "\n",
    "        model.fit(train_data.toarray(), train_labels)\n",
    "        pred = model.predict(test_data.toarray())\n",
    "\n",
    "        dataset[0][name]=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predictions to csv\n",
    "for dataset in datasets:\n",
    "    dataset[0].to_csv(\"results/predictions/\"+dataset[1]+\".csv\")\n",
    "    frame = pd.DataFrame(columns=[\"dataset_name\",\"priv_val\",\"unpriv_val\",\"protected_att\"])\n",
    "    dataset_name = dataset[1]\n",
    "    priv_val = dataset[2]\n",
    "    unpriv_val = dataset[3]\n",
    "    protected_att=dataset[4]\n",
    "    frame.loc[len(frame)] = [dataset_name, priv_val, unpriv_val, protected_att]\n",
    "    frame.to_csv(\"results/predictions/\"+dataset[1]+\"_attributes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load predictions from csv\n",
    "dataset_names = [\"Adult_Income\", \"Default_Of_Credit\", \"German_Credit\", \"Heart_Diseases\", \"Heart_Failure\", \"Ricci_vs_Stefano\", \"Student_Performance\"]\n",
    "definitions_names = [\"statistical parity\", \"predictive parity\", \"negative predictive parity\", \"equal opportunity\", \"predictive equality\", \"overall accuracy equality\", \"treatment equality\"]\n",
    "classifiers = [DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42),SVC(),AdaBoostClassifier(),KNeighborsClassifier(5), GaussianNB(), XGBClassifier()]\n",
    "model_names = []\n",
    "\n",
    "for model in classifiers:\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    model_names.append(name)\n",
    "\n",
    "datasets=[]\n",
    "for dataset in dataset_names:\n",
    "    test = pd.read_csv(\"results/predictions/\"+dataset+\".csv\")\n",
    "    att = pd.read_csv(\"results/predictions/\"+dataset+\"_attributes.csv\")\n",
    "    dataset_name = att[\"dataset_name\"].item()\n",
    "    priv_val = att[\"priv_val\"].item()\n",
    "    unpriv_val = att[\"unpriv_val\"].item()\n",
    "    protected_att=att[\"protected_att\"].item()\n",
    "    datasets.append((test, dataset_name, priv_val, unpriv_val, protected_att))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = pd.DataFrame(columns=[\"dataset\", \"model\", \"accuracy\"])\n",
    "for dataset in datasets:\n",
    "    for name in model_names:\n",
    "        acc = accuracy_score(dataset[0][\"class\"],dataset[0][name])\n",
    "        accuracy.loc[len(accuracy)]=[dataset[1],name,acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy for each dataset\n",
    "\n",
    "plot = (p9.ggplot(data= accuracy, mapping = p9.aes(x=\"model\", y=\"accuracy\")) \n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.facet_grid(\".~dataset\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Accuracy\", title=(\"Accuracy for each dataset\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/\"+\"accuracy_overall.png\", height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average accuracy\n",
    "plot = (p9.ggplot(data= accuracy.groupby([\"model\"], as_index=False).mean(), mapping = p9.aes(x=\"model\", y=\"accuracy\")) \n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Accuracy\", title=(\"Avarage Accuracy\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/\"+\"accuracy_avarage.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Fairness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing classifiers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = FairnessTester()\n",
    "results=[]\n",
    "\n",
    "\n",
    "#run the tester on the results and safe results in dataframe\n",
    "for dataset in datasets:\n",
    "    test = dataset[0]\n",
    "    dataset_name = dataset[1]\n",
    "    priv_val = dataset[2]\n",
    "    unpriv_val = dataset[3]\n",
    "    protected_att=dataset[4]\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    for name in model_names:\n",
    "        tester.setup(test, protected_att, priv_val, unpriv_val, name)\n",
    "        result_dic = {\"model\": name}\n",
    "        result_dic.update(tester.confusion_based_dic_priv())\n",
    "        result_df= result_df.append(result_dic, ignore_index=True)\n",
    "        \n",
    "        result_dic = {\"model\": name}\n",
    "        result_dic.update(tester.confusion_based_dic_unpriv())\n",
    "        result_df= result_df.append(result_dic, ignore_index=True)\n",
    "\n",
    "definitions_names = list(tester.confuison_based_dic().keys())\n",
    "\n",
    "#safe results as csv\n",
    "#result_df.to_csv(\"results/fairness/\"+dataset_name+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fairness Results from CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all data into a list of results\n",
    "dataset_names = [\"Adult_Income\", \"Default_Of_Credit\", \"German_Credit\", \"Heart_Diseases\", \"Heart_Failure\", \"Ricci_vs_Stefano\", \"Student_Performance\"]\n",
    "definitions_names = [\"statistical parity\", \"predictive parity\", \"negative predictive parity\", \"equal opportunity\", \"predictive equality\", \"overall accuracy equality\", \"treatment equality\"]\n",
    "classifiers = [DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42),SVC(),AdaBoostClassifier(),KNeighborsClassifier(5), GaussianNB(), XGBClassifier()]\n",
    "model_names = []\n",
    "\n",
    "for model in classifiers:\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    model_names.append(name)\n",
    "\n",
    "#create list of results\n",
    "results=[]\n",
    "for dataset in dataset_names:\n",
    "    result = pd.read_csv(\"results/fairness/\"+dataset+\".csv\")\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Results for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform to better selectable format for all datasets\n",
    "full_result_df = pd.DataFrame()\n",
    "i=0\n",
    "for result in results:\n",
    "    result_df = result\n",
    "    for model in model_names:\n",
    "        for defi in definitions_names:\n",
    "            for group in [\"priv\", \"unpriv\"]:    \n",
    "                if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "                    continue \n",
    "                dic = {}\n",
    "                dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "                dic[\"group\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"group\"].item()\n",
    "                dic[\"definition\"] = defi\n",
    "                dic[\"result\"]= result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][defi].item()\n",
    "                dic[\"dataset\"] = dataset_names[i]\n",
    "                full_result_df = full_result_df.append(dic, ignore_index=True)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate differences for all datasets\n",
    "\n",
    "full_differences = pd.DataFrame()\n",
    "i=0\n",
    "for result in results:\n",
    "    result_df = result\n",
    "    differences = pd.DataFrame()\n",
    "    for model in model_names:\n",
    "        for defi in definitions_names:   \n",
    "            if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "                continue          \n",
    "            dic = {}\n",
    "            dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "            x = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"priv\")][defi].item()\n",
    "            y = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"unpriv\")][defi].item()\n",
    "            dic[\"definition\"] = defi\n",
    "            diff = abs(x-y)\n",
    "            dic[\"difference\"]= diff\n",
    "            if (diff <=0.1): # implementing threshold for consideration of fairness/unfairness\n",
    "                dic[\"fairness\"]=\"Fair\"\n",
    "            else:\n",
    "                dic[\"fairness\"]=\"Unfair\"\n",
    "            differences = differences.append(dic, ignore_index=True)\n",
    "    differences[\"dataset\"]=dataset_names[i]\n",
    "    full_differences = full_differences.append(differences, ignore_index=True)\n",
    "    \n",
    "    i+=1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate ratio for all datasets\n",
    "full_ratio = pd.DataFrame()\n",
    "i=0\n",
    "for result in results:\n",
    "    result_df = result\n",
    "    ratio = pd.DataFrame()\n",
    "    for model in model_names:\n",
    "        for defi in definitions_names:   \n",
    "            if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "                continue          \n",
    "            dic = {}\n",
    "            dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "            x = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"priv\")][defi].item()\n",
    "            y = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"unpriv\")][defi].item()\n",
    "            dic[\"definition\"] = defi\n",
    "            if (x!=0.0):\n",
    "                rat = (y/x)  \n",
    "            else:\n",
    "                rat = 0 \n",
    "            dic[\"ratio\"]= rat\n",
    "            if (rat <=0.8 or rat>=1.25): # implementing threshold for consideration of fairness/unfairness\n",
    "                dic[\"fairness\"]=\"Unfair\"\n",
    "            else:\n",
    "                dic[\"fairness\"]=\"Fair\"\n",
    "            ratio = ratio.append(dic, ignore_index=True)\n",
    "    ratio[\"dataset\"]=dataset_names[i]\n",
    "    full_ratio = full_ratio.append(ratio, ignore_index=True)\n",
    "    \n",
    "    i+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average  difference vs ratio vs accuracy in DataFrame\n",
    "\n",
    "average_differences = full_differences.groupby([\"dataset\",\"model\"], as_index=False).mean()\n",
    "average_ratio = full_ratio.groupby([\"dataset\",\"model\"], as_index=False).mean()\n",
    "\n",
    "accuracy_fairness = pd.DataFrame(columns=[\"dataset\",\"model\" ,\"difference\", \"ratio\", \"accuracy\"])\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    for model in model_names:\n",
    "        acc = accuracy.loc[(accuracy[\"dataset\"]==dataset)&(accuracy[\"model\"]==model)][\"accuracy\"].item()\n",
    "        diff =  average_differences.loc[(average_differences[\"dataset\"]==dataset)&(average_differences[\"model\"]==model)][\"difference\"].item()\n",
    "        rat = average_ratio.loc[(average_ratio[\"dataset\"]==dataset)&(average_ratio[\"model\"]==model)][\"ratio\"].item()\n",
    "        accuracy_fairness.loc[len(accuracy_fairness)]=[dataset,model,diff,rat,acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall plot for all definitions with color\n",
    "#skip treatment equality as it will destory the scales\n",
    "for dataset_name in dataset_names:\n",
    "    plot = (p9.ggplot(data= full_result_df.loc[full_result_df[\"dataset\"]==dataset_name], mapping = p9.aes(x=\"model\", y=\"result\", fill=\"group\")) \n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.facet_grid(\".~definition\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Results\", title=(\" Complete results for \" + dataset_name))\n",
    "        )\n",
    "    plot.save(filename=\"plots/full_results/\"+dataset_name+\"_complete_color.png\", height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy vs fariness scatterplot\n",
    "plot = (p9.ggplot(data= accuracy_fairness, mapping = p9.aes(x=\"difference\", y=\"accuracy\", color=\"model\")) \n",
    "        + p9.geom_point()\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"Average difference\", y= \"Accuracy\", title=\"Average difference in fairness definitions vs Accuracy\")\n",
    "        )\n",
    "plot.save(filename=\"plots/difference/fairness_vs_accuracy_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot differences for each dataset\n",
    "for dataset_name in dataset_names: \n",
    "    plot = (p9.ggplot(data= full_differences.loc[full_differences[\"dataset\"]==dataset_name], mapping = p9.aes(x=\"model\", y=\"difference\", fill=\"fairness\")) \n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.facet_grid(\".~definition\", space=\"free_x\", scales=\"fixed\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Differences for \" + dataset_name))\n",
    "        + p9.ylim(0,1)\n",
    "        + p9.scale_fill_manual(values=(\"green\",\"red\"))\n",
    "        )\n",
    "    plot.save(filename=\"plots/difference/datasets/\"+dataset_name+\"_differences_scaled.png\", height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot differences for each classifiers over all datasets\n",
    "for model in model_names: \n",
    "    plot = (p9.ggplot(data= full_differences.groupby([\"model\",\"definition\"], as_index=False).mean().loc[full_differences[\"model\"]==model], \n",
    "            mapping = p9.aes(x=\"definition\", y=\"difference\",)) \n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        #+ p9.facet_grid(\".~model\", space=\"free_x\", scales=\"fixed\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"Definition\", y= \"Differences\", title=(\"Differences for \" + model + \" over all datasets\"))\n",
    "        + p9.ylim(0,1)\n",
    "        + p9.scale_fill_manual(values=(\"green\",\"red\"))\n",
    "        )\n",
    "    plot.save(filename=\"plots/difference/models/\"+model+\"_differences.png\", height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#differences for each definition on all datasets\n",
    "for definition in definitions_names:\n",
    "        if definition==\"treatment equality\":\n",
    "            continue\n",
    "        plot = (p9.ggplot(data= full_differences.loc[(full_differences[\"definition\"]==definition)],\n",
    "                        mapping = p9.aes(x=\"model\", y=\"difference\", fill=\"fairness\")) \n",
    "                + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "                + p9.geom_col(position=\"dodge\")\n",
    "                + p9.facet_wrap(\"dataset\")\n",
    "                + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "                + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Differences for \"+definition))\n",
    "                + p9.ylim(0,1)\n",
    "                + p9.scale_fill_manual(values=({\"Fair\":\"green\",\"Unfair\":\"red\"}))\n",
    "                )\n",
    "        plot.save(filename=\"plots/difference/definitions/\"+definition+\"_differences.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplots for difference on classifiers over all datasets for each definition\n",
    "\n",
    "plot = (p9.ggplot(data= full_differences, mapping = p9.aes(x=\"model\", y= \"difference\"))\n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.facet_grid(\".~definition\",space=\"free_x\", scales=\"fixed\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Boxplots for all ML models\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/difference/boxplot_models_diff.png\",height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for difference of classifiers over all definitions\n",
    "\n",
    "plot = (p9.ggplot(data= full_differences, mapping = p9.aes(x=\"model\", y= \"difference\"))\n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Boxplots for all ML models\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/difference/boxplot_models_overall_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot mean and range of differences \n",
    "\n",
    "plot = (p9.ggplot(data= full_differences, mapping = p9.aes(x=\"model\", y= \"difference\"))\n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.stat_summary(fun_y = np.mean, fun_ymin=np.min, fun_ymax=np.max)\n",
    "        + p9.facet_grid(\".~definition\",space=\"free_x\", scales=\"fixed\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Average and range for all ML models\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/difference/average_models_diff.png\", width=17, height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for difference over definitions\n",
    "plot = (p9.ggplot(data= full_differences, mapping = p9.aes(x=\"definition\", y= \"difference\"))\n",
    "        + p9.geom_hline(yintercept = 0.1, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"Fairness Definitions\", y= \"Differences\", title=(\"Boxplots for all definitions\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/difference/boxplot_definitions_diff.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy vs fariness ratio scatterplot\n",
    "plot = (p9.ggplot(data= accuracy_fairness, mapping = p9.aes(x=\"ratio\", y=\"accuracy\", color=\"model\")) \n",
    "        + p9.geom_point()\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"Average ratio\", y= \"Accuracy\", title=\"Average Fairness (ratio) vs Accuracy\")\n",
    "        )\n",
    "plot.save(filename=\"plots/ratio/fairness_vs_accuracy_ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ratio for each dataset\n",
    "for dataset_name in dataset_names: \n",
    "    plot = (p9.ggplot(data= full_ratio.loc[full_ratio[\"dataset\"]==dataset_name], mapping = p9.aes(x=\"model\", y=\"ratio\", fill=\"fairness\")) \n",
    "        + p9.geom_hline(yintercept = 0.8, color=\"red\")\n",
    "        + p9.geom_hline(yintercept = 1, color=\"green\")\n",
    "        + p9.geom_hline(yintercept = 1.25, color=\"red\")\n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        + p9.facet_grid(\".~definition\", space=\"free_x\", scales=\"fixed\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Ratio\", title=(\"Ratio for \" + dataset_name))\n",
    "        + p9.scale_fill_manual(values=(\"green\",\"red\"))\n",
    "        )\n",
    "    plot.save(filename=\"plots/ratio/datasets/\"+dataset_name+\"_ratio.png\", height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ratio for each classifiers over all datasets\n",
    "for model in model_names: \n",
    "    plot = (p9.ggplot(data= full_ratio.groupby([\"model\",\"definition\"], as_index=False).mean().loc[full_ratio[\"model\"]==model], \n",
    "            mapping = p9.aes(x=\"definition\", y=\"ratio\",)) \n",
    "        + p9.geom_hline(yintercept = 0.8, color=\"red\")\n",
    "        + p9.geom_hline(yintercept = 1, color=\"green\")\n",
    "        + p9.geom_hline(yintercept = 1.25, color=\"red\")\n",
    "        + p9.geom_col(position=\"dodge\")\n",
    "        #+ p9.facet_grid(\".~model\", space=\"free_x\", scales=\"fixed\") \n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"Definition\", y= \"Ratio\", title=(\"Ratio for \" + model + \" over all datasets\"))\n",
    "        )\n",
    "    plot.save(filename=\"plots/ratio/models/\"+model+\"_ratio.png\", height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio for each definition on all datasets\n",
    "for definition in definitions_names:\n",
    "        if definition==\"treatment equality\":\n",
    "            continue\n",
    "        plot = (p9.ggplot(data= full_ratio.loc[(full_ratio[\"definition\"]==definition)],\n",
    "                        mapping = p9.aes(x=\"model\", y=\"ratio\", fill=\"fairness\")) \n",
    "                + p9.geom_hline(yintercept = 0.8, color=\"red\")\n",
    "                + p9.geom_hline(yintercept = 1, color=\"green\")\n",
    "                + p9.geom_hline(yintercept = 1.25, color=\"red\")\n",
    "                + p9.geom_col(position=\"dodge\")\n",
    "                + p9.facet_wrap(\"dataset\")\n",
    "                + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "                + p9.labs(x=\"ML Models\", y= \"Ratio\", title=(\"Ratio for \"+definition))\n",
    "                \n",
    "                + p9.scale_fill_manual(values=({\"Fair\":\"green\",\"Unfair\":\"red\"}))\n",
    "                )\n",
    "        plot.save(filename=\"plots/ratio/definitions/\"+definition+\"_ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplots for difference on classifiers over all datasets for each definition\n",
    "\n",
    "plot = (p9.ggplot(data= full_ratio, mapping = p9.aes(x=\"model\", y= \"ratio\"))\n",
    "        + p9.geom_hline(yintercept = 0.8, color=\"red\")\n",
    "        + p9.geom_hline(yintercept = 1, color=\"green\")\n",
    "        + p9.geom_hline(yintercept = 1.25, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.facet_grid(\".~definition\",space=\"free_x\", scales=\"fixed\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Ratio\", title=(\"Boxplots of Ratio for all ML models\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/ratio/boxplot_models_ratio.png\",height=4 , width = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for difference of classifiers over all definitions\n",
    "\n",
    "plot = (p9.ggplot(data= full_ratio, mapping = p9.aes(x=\"model\", y= \"ratio\"))\n",
    "        + p9.geom_hline(yintercept = 0.8, color=\"red\")\n",
    "        + p9.geom_hline(yintercept = 1, color=\"green\")\n",
    "        + p9.geom_hline(yintercept = 1.25, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"ML Models\", y= \"Ratio\", title=(\"Boxplots of Ratio for all ML models over all Datasets\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/ratio/boxplot_models_overall_ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for ratio over definitions\n",
    "plot = (p9.ggplot(data= full_ratio, mapping = p9.aes(x=\"definition\", y= \"ratio\"))\n",
    "        + p9.geom_hline(yintercept = 0.8, color=\"red\")\n",
    "        + p9.geom_hline(yintercept = 1, color=\"green\")\n",
    "        + p9.geom_hline(yintercept = 1.25, color=\"red\")\n",
    "        + p9.geom_boxplot(color=\"black\", fill=\"darkgrey\")\n",
    "        + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "        + p9.labs(x=\"Fairness Definitions\", y= \"Ratio\", title=(\"Boxplots for Ratio over all definitions\"))\n",
    "        )\n",
    "plot.save(filename=\"plots/ratio/boxplot_definitions_ratio.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
