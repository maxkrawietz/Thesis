{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Setups"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotnine as p9\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import transformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from fairnesTester import FairnessTester\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fairnesTester import FairnessTester\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#pipelines and transformer\n",
    "\n",
    "cat_trans = Pipeline(steps=[\n",
    "    (\"selector\", transformer.DataSelector(\"object\")),\n",
    "    (\"one_hot\", preprocessing.OneHotEncoder())\n",
    "])\n",
    "num_trans = Pipeline(steps=[\n",
    "    (\"selector\", transformer.DataSelector(\"number\")),\n",
    "    (\"scaler\", StandardScaler() )\n",
    "])\n",
    "\n",
    "pre_pipe = FeatureUnion(transformer_list=[\n",
    "    (\"cat\", cat_trans),\n",
    "    (\"num\", num_trans)\n",
    "])\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "del_nan = transformer.DeleteNAN(\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing and preparing the data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adult Income Data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/adult.data\"\n",
    "names = [\"age\", \"workclass\", \"fnlwgt\",\"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
    "train = pd.read_csv(filename, names=names)\n",
    "test = test = pd.read_csv(\"Datasets/adult.test\", names=names)\n",
    "\n",
    "del_nan.set_nan_char(\" ?\")\n",
    "train = del_nan.transform(train)\n",
    "test = del_nan.transform(test)\n",
    "\n",
    "train[\"class\"] = lb.fit_transform(train[\"class\"])\n",
    "test[\"class\"] = lb.fit_transform(test[\"class\"])\n",
    "\n",
    "train_data = pre_pipe.fit_transform(train.drop(\"class\", axis=1))\n",
    "train_labels = train[\"class\"]\n",
    "\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "test_labels = test[\"class\"]\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Adult_Income\"\n",
    "priv_val = \" Male\"\n",
    "unpriv_val = \" Female\"\n",
    "protected_att = \"sex\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## German Credit Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/german.data\"\n",
    "names = [\"status existing account\",\"duration\", \"credit history\", \"purpose\", \"credit amount\", \"savings\", \"employment since\", \"installment rate\", \"sex\", \"other debtors\", \"residence since\", \"property\", \"age\", \"installment plans\", \"housing\", \"num existing credits\", \"job\", \"no of pople liable\", \"telephone\", \"foreign worker\", \"class\" ]\n",
    "data = pd.read_csv(filename, sep=\" \", names =names)\n",
    "\n",
    "data[\"class\"] = lb.fit_transform(data[\"class\"])\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data, data[\"class\"], random_state=42)\n",
    "\n",
    "train_data = pre_pipe.fit_transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "#transform for Fairness tester\n",
    "test[\"sex\"].replace([\"A91\",\"A93\", \"A94\"],\"m\",inplace=True)\n",
    "test[\"sex\"].replace([\"A92\",\"A95\"],\"f\",inplace=True)\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"German_Credit\"\n",
    "priv_val = \"m\"\n",
    "unpriv_val = \"f\"\n",
    "protected_att = \"sex\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Default of Credit Card Payments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/default of credit.xls\"\n",
    "data_inp = pd.read_excel(filename, dtype={\"X1\": int,\"X2\": object,\"X3\": object,\"X4\": object,\"X5\": object,\"X6\": object,\"X7\": object,\"X8\": object,\"X9\": object,\"X10\": object,\"X11\": object,\"X12\": int,\"X13\": int,\"X14\": int,\"X15\": int,\"X16\": int,\"X17\": int,\"X23\": int,\"X18\": int,\"X19\": int,\"X20\": int,\"X21\": int,\"X22\": int})\n",
    "\n",
    "data_inp = data_inp.rename(columns={\"Y\": \"class\"})\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Default_Of_Credit\"\n",
    "priv_val = 1 #male\n",
    "unpriv_val = 2 #female\n",
    "protected_att = \"X2\"\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rici vs Stefano Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/ricci.csv\"\n",
    "data_inp = pd.read_csv(filename).drop(\"Unnamed: 0\", axis=1)\n",
    "#applicants with combine >= 70 pass\n",
    "#read paper Did the Results of Promotion Exams Have a Disparate Impact on Minorities? Using Statistical Evidence in Ricci v. DeStefano\n",
    "data_inp.rename(columns={\"Combine\": \"class\"}, inplace=True)\n",
    "\n",
    "data_inp.loc[(data_inp[\"class\"]<70), \"class\"] = 0\n",
    "data_inp.loc[data_inp[\"class\"]>=70, \"class\"] = 1\n",
    "\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "#transform for Fairness tester\n",
    "test[\"Race\"].replace([\"H\",\"B\",],\"NW\",inplace=True)\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Ricci_vs_Stefano\"\n",
    "priv_val = \"W\" #white\n",
    "unpriv_val = \"NW\" #not white\n",
    "protected_att = \"Race\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heart Disease Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/processed.cleveland.data\"\n",
    "names = [\"age\", \"sex\", 3,4,5,6,7,8,9,10,11,12,13,\"class\"]\n",
    "data_inp = pd.read_csv(filename, names=names)\n",
    "\n",
    "\n",
    "data_inp.loc[data_inp[\"class\"]>=1, \"class\"] = 1 #existing heart disase\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Heart_Diseases\"\n",
    "priv_val = 1 #male\n",
    "unpriv_val = 0 #female\n",
    "protected_att = \"sex\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heart Failure Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/heart_failure.csv\"\n",
    "data_inp = pd.read_csv(filename)\n",
    "data_inp.rename(columns={\"DEATH_EVENT\":\"class\"}, inplace=True)\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Heart_Failure\"\n",
    "priv_val = 1 #male\n",
    "unpriv_val = 0 #female\n",
    "protected_att = \"sex\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Student Performance Data Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"Datasets/student-por.csv\"\n",
    "data_inp = pd.read_csv(filename, sep=\";\")\n",
    "\n",
    "data_inp.drop([\"G1\",\"G2\"],axis=1,inplace=True)\n",
    "data_inp.rename(columns={\"G3\":\"class\"},inplace=True)\n",
    "\n",
    "data_inp.loc[(data_inp[\"class\"]<10), \"class\"] = 0 #failed\n",
    "data_inp.loc[data_inp[\"class\"]>=10, \"class\"] = 1 #passed\n",
    "\n",
    "\n",
    "data, test, train_labels, test_labels = train_test_split(data_inp, data_inp[\"class\"], random_state=42)\n",
    "\n",
    "\n",
    "pre_pipe.fit(data_inp.drop(\"class\", axis=1))\n",
    "train_data = pre_pipe.transform(data.drop(\"class\", axis=1))\n",
    "test_data = pre_pipe.transform(test.drop(\"class\", axis=1))\n",
    "\n",
    "\n",
    "#attribute for Fairness tester\n",
    "dataset_name = \"Student_Performance\"\n",
    "priv_val = \"M\" #male\n",
    "unpriv_val = \"F\" #female\n",
    "protected_att = \"sex\"\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classifiers = [DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42),SVC(),AdaBoostClassifier(),KNeighborsClassifier(5), GaussianNB(), XGBClassifier()]\n",
    "model_names = []\n",
    "\n",
    "for model in classifiers:\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    print(name)\n",
    "    model_names.append(name)\n",
    "\n",
    "    model.fit(train_data.toarray(), train_labels)\n",
    "    pred = model.predict(test_data.toarray())\n",
    "\n",
    "    test[name]=pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing for Fairness\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## testing classifiers list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tester = FairnessTester()\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for name in model_names:\n",
    "    tester.setup(test, protected_att, priv_val, unpriv_val, name)\n",
    "    result_dic = {\"model\": name}\n",
    "    result_dic.update(tester.confusion_based_dic_priv())\n",
    "    result_df= result_df.append(result_dic, ignore_index=True)\n",
    "    \n",
    "    result_dic = {\"model\": name}\n",
    "    result_dic.update(tester.confusion_based_dic_unpriv())\n",
    "    result_df= result_df.append(result_dic, ignore_index=True)\n",
    "definitions_names = list(tester.confuison_based_dic().keys())\n",
    "\n",
    "result_df.to_csv(\"results/\"+dataset_name+\".csv\")\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "#read results from csv\n",
    "\n",
    "dataset_name = \"Student_Performance\"\n",
    "\n",
    "result_df = pd.read_csv(\"results/\"+dataset_name+\".csv\")\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(random_state=42),RandomForestClassifier(random_state=42),SVC(),AdaBoostClassifier(),KNeighborsClassifier(5), GaussianNB(), XGBClassifier()]\n",
    "model_names = []\n",
    "\n",
    "for model in classifiers:\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    model_names.append(name)\n",
    "\n",
    "definitions_names = [\"statistical parity\", \"predictive parity\", \"negative predictive parity\", \"equal opportunity\", \"predictive equality\", \"overall accuracy equality\", \"treatment equality\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "#fit all data into better selectable format(for complete plot of all definitions)\n",
    "\n",
    "full_result_df = pd.DataFrame()\n",
    "for model in model_names:\n",
    "    for defi in definitions_names:\n",
    "        for group in [\"priv\", \"unpriv\"]:    \n",
    "            #if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "            #    continue          \n",
    "            dic = {}\n",
    "            dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "            dic[\"group\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"group\"].item()\n",
    "            dic[\"definition\"] = defi\n",
    "            dic[\"result\"]= result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][defi].item()\n",
    "            full_result_df = full_result_df.append(dic, ignore_index=True)\n",
    "\n",
    "full_result_df\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>group</th>\n",
       "      <th>definition</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>priv</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.934426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>unpriv</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>priv</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.807018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>unpriv</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.967391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>priv</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>unpriv</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>priv</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.754098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>unpriv</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.931373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>priv</td>\n",
       "      <td>treatment equality</td>\n",
       "      <td>6.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>unpriv</td>\n",
       "      <td>treatment equality</td>\n",
       "      <td>15.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model   group                  definition     result\n",
       "0   DecisionTreeClassifier    priv          statistical parity   0.934426\n",
       "1   DecisionTreeClassifier  unpriv          statistical parity   0.901961\n",
       "2   DecisionTreeClassifier    priv           predictive parity   0.807018\n",
       "3   DecisionTreeClassifier  unpriv           predictive parity   0.967391\n",
       "4   DecisionTreeClassifier    priv  negative predictive parity   0.250000\n",
       "..                     ...     ...                         ...        ...\n",
       "93           XGBClassifier  unpriv         predictive equality   0.500000\n",
       "94           XGBClassifier    priv   overall accuracy equality   0.754098\n",
       "95           XGBClassifier  unpriv   overall accuracy equality   0.931373\n",
       "96           XGBClassifier    priv          treatment equality   6.125000\n",
       "97           XGBClassifier  unpriv          treatment equality  15.666667\n",
       "\n",
       "[98 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# get differences from data\n",
    "\n",
    "differences = pd.DataFrame()\n",
    "\n",
    "for model in model_names:\n",
    "    for defi in definitions_names:   \n",
    "        if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "            continue          \n",
    "        dic = {}\n",
    "        dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "        x = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"priv\")][defi].item()\n",
    "        y = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"unpriv\")][defi].item()\n",
    "        dic[\"definition\"] = defi\n",
    "        diff = abs(x-y)\n",
    "        dic[\"difference\"]= diff\n",
    "        if (diff <=0.1): # implementing threshold for consideration of fairness/unfairness\n",
    "            dic[\"fairness\"]=\"Fair\"\n",
    "        else:\n",
    "            dic[\"fairness\"]=\"Unfair\"\n",
    "        differences = differences.append(dic, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "differences"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>definition</th>\n",
       "      <th>difference</th>\n",
       "      <th>fairness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.032465</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.160374</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.151077</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.082289</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVC</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.019769</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVC</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.111808</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVC</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SVC</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SVC</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.095307</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.123839</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>0.100304</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.174060</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.036162</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.132376</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.160881</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.036644</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>0.037125</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.015751</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>statistical parity</td>\n",
       "      <td>0.078914</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>predictive parity</td>\n",
       "      <td>0.130972</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>negative predictive parity</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>0.090534</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>predictive equality</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>overall accuracy equality</td>\n",
       "      <td>0.177274</td>\n",
       "      <td>Unfair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                  definition  difference fairness\n",
       "0   DecisionTreeClassifier          statistical parity    0.032465     Fair\n",
       "1   DecisionTreeClassifier           predictive parity    0.160374   Unfair\n",
       "2   DecisionTreeClassifier  negative predictive parity    0.250000   Unfair\n",
       "3   DecisionTreeClassifier           equal opportunity    0.008033     Fair\n",
       "4   DecisionTreeClassifier         predictive equality    0.541667   Unfair\n",
       "5   DecisionTreeClassifier   overall accuracy equality    0.151077   Unfair\n",
       "6   RandomForestClassifier          statistical parity    0.003214     Fair\n",
       "7   RandomForestClassifier           predictive parity    0.103333   Unfair\n",
       "8   RandomForestClassifier  negative predictive parity    1.000000   Unfair\n",
       "9   RandomForestClassifier           equal opportunity    0.021277     Fair\n",
       "10  RandomForestClassifier         predictive equality    0.083333     Fair\n",
       "11  RandomForestClassifier   overall accuracy equality    0.082289     Fair\n",
       "12                     SVC          statistical parity    0.019769     Fair\n",
       "13                     SVC           predictive parity    0.111808   Unfair\n",
       "14                     SVC  negative predictive parity    0.000000     Fair\n",
       "15                     SVC           equal opportunity    0.009770     Fair\n",
       "16                     SVC         predictive equality    0.083333     Fair\n",
       "17                     SVC   overall accuracy equality    0.111700   Unfair\n",
       "18      AdaBoostClassifier          statistical parity    0.095307     Fair\n",
       "19      AdaBoostClassifier           predictive parity    0.123839   Unfair\n",
       "20      AdaBoostClassifier  negative predictive parity    0.128571   Unfair\n",
       "21      AdaBoostClassifier           equal opportunity    0.100304   Unfair\n",
       "22      AdaBoostClassifier         predictive equality    0.125000   Unfair\n",
       "23      AdaBoostClassifier   overall accuracy equality    0.174060   Unfair\n",
       "24    KNeighborsClassifier          statistical parity    0.036162     Fair\n",
       "25    KNeighborsClassifier           predictive parity    0.132376   Unfair\n",
       "26    KNeighborsClassifier  negative predictive parity    0.416667   Unfair\n",
       "27    KNeighborsClassifier           equal opportunity    0.050586     Fair\n",
       "28    KNeighborsClassifier         predictive equality    0.166667   Unfair\n",
       "29    KNeighborsClassifier   overall accuracy equality    0.160881   Unfair\n",
       "30              GaussianNB          statistical parity    0.036644     Fair\n",
       "31              GaussianNB           predictive parity    0.044643     Fair\n",
       "32              GaussianNB  negative predictive parity    0.371795   Unfair\n",
       "33              GaussianNB           equal opportunity    0.037125     Fair\n",
       "34              GaussianNB         predictive equality    0.208333   Unfair\n",
       "35              GaussianNB   overall accuracy equality    0.015751     Fair\n",
       "36           XGBClassifier          statistical parity    0.078914     Fair\n",
       "37           XGBClassifier           predictive parity    0.130972   Unfair\n",
       "38           XGBClassifier  negative predictive parity    0.238095   Unfair\n",
       "39           XGBClassifier           equal opportunity    0.090534     Fair\n",
       "40           XGBClassifier         predictive equality    0.250000   Unfair\n",
       "41           XGBClassifier   overall accuracy equality    0.177274   Unfair"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# get ratio from data\n",
    "\n",
    "ratio = pd.DataFrame()\n",
    "\n",
    "for model in model_names:\n",
    "    for defi in definitions_names:   \n",
    "        if defi == \"treatment equality\":   #skip treatment equality as it will destroy the scales for full plot\n",
    "            continue          \n",
    "        dic = {}\n",
    "        dic[\"model\"] = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==group)][\"model\"].item()\n",
    "        x = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"priv\")][defi].item()\n",
    "        y = result_df.loc[(result_df[\"model\"]==model)&(result_df[\"group\"]==\"unpriv\")][defi].item()\n",
    "        dic[\"definition\"] = defi\n",
    "        if (x == 0): #avoid dividing by zero\n",
    "            dic[\"ratio\"]= 0\n",
    "        else:\n",
    "            dic[\"ratio\"]= y/x\n",
    "        \n",
    "        ratio = ratio.append(dic, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "#overall plot for all definitions with color\n",
    "#skip treatment equality as it will destory the scales\n",
    "plot = (p9.ggplot(data= full_result_df.drop(full_result_df[full_result_df.definition==\"treatment equality\"].index), mapping = p9.aes(x=\"model\", y=\"result\", fill=\"group\")) \n",
    "    + p9.geom_col(position=\"dodge\")\n",
    "    + p9.facet_grid(\".~definition\") \n",
    "    + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "    + p9.labs(x=\"ML Models\", y= \"Results\", title=(\" Complete results for \" + dataset_name))\n",
    "    )\n",
    "plot.save(filename=\"plots/\"+dataset_name+\"/\"+dataset_name+\"_complete_color.png\", height=4 , width = 17)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 17 x 4 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_complete_color.png\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# plot differences\n",
    "plot = (p9.ggplot(data= differences, mapping = p9.aes(x=\"model\", y=\"difference\", fill=\"fairness\")) \n",
    "    + p9.geom_col(position=\"dodge\")\n",
    "    + p9.facet_grid(\".~definition\", space=\"free_x\", scales=\"fixed\") \n",
    "    + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "    + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Differences for \" + dataset_name))\n",
    "    + p9.ylim(0,1)\n",
    "    + p9.scale_fill_manual(values=(\"green\",\"red\"))\n",
    "    )\n",
    "plot.save(filename=\"plots/\"+dataset_name+\"/\"+dataset_name+\"_differences_scaled.png\", height=4 , width = 17)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 17 x 4 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_differences_scaled.png\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "#plot differences for each definition\n",
    "plots = []\n",
    "for definition in definitions_names:\n",
    "\n",
    "    plot = (p9.ggplot(data= differences.loc[differences[\"definition\"]==definition], mapping = p9.aes(x=\"model\", y=\"difference\", fill=\"fairness\")) \n",
    "    + p9.geom_col(position=\"dodge\")\n",
    "    + p9.theme(axis_text_x = p9.element_text(angle=90))\n",
    "    + p9.labs(x=\"ML Models\", y= \"Differences\", title=(\"Differences for \"+definition+\" on \" + dataset_name + \" Dataset\"))\n",
    "    + p9.ylim(0,1)\n",
    "    + p9.scale_fill_manual(values=({\"Fair\":\"green\",\"Unfair\":\"red\"}))\n",
    "    )\n",
    "    plots.append(plot)\n",
    "i=0\n",
    "\n",
    "for plot in plots:\n",
    "    if definitions_names[i]==\"treatment equality\":\n",
    "        continue\n",
    "    plot.save(filename=\"plots/\"+dataset_name+\"/\"+dataset_name+\"_\"+definitions_names[i]+\"_difference.png\")\n",
    "    i+=1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_statistical parity_difference.png\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_predictive parity_difference.png\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_negative predictive parity_difference.png\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_equal opportunity_difference.png\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_predictive equality_difference.png\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/maxk/.local/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: plots/Student_Performance/Student_Performance_overall accuracy equality_difference.png\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}